{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch over hyperparameters of MNIST autoencoder with sparsity and participation constraints\n",
    "\n",
    "In this notebook, we write a tensorflow wrapper for sklearn, such that we can use the sklearn gridsearch framework to tune the hyperparameters of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "## Download and read mnist data\n",
    "mnist = input_data.read_data_sets('mnist_data', one_hot=True)\n",
    "\n",
    "### Helper functions\n",
    "def sshift(c, k=100.):\n",
    "    \"\"\"How to shift the sigmoid along the x-axis, such that sigmoid(0) = c.\n",
    "\n",
    "    Args:\n",
    "        c: y-value of sigmoid(0).\n",
    "        k: Stretching factor of sigmoid.\n",
    "\n",
    "    Returns:\n",
    "        Returns x_s such that sigmoid(x-x_s) = c for x = 0. Note, that the original sigmoid has the property that\n",
    "        sigmoid(x) = 0.5 for x = 0.\n",
    "    \"\"\"\n",
    "    return 1./k * tf.log((1.-c)/c)\n",
    "\n",
    "def sigmoid(x, x_s, k=100.):\n",
    "    \"\"\"Compute a sigmoid fuction.\n",
    "\n",
    "    Args:\n",
    "        x: Input to sigmoid\n",
    "        c: y-value of sigmoid(0).\n",
    "        k: Stretching factor of sigmoid.\n",
    "\n",
    "    Returns:\n",
    "        Returns 1/(1+exp(-k*(x-x_s)))\n",
    "    \"\"\"\n",
    "    return tf.div(1.,tf.add(1.,tf.exp(-k * (x-x_s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderMNIST(BaseEstimator):\n",
    "    \"\"\" A class that adheres the scikit-learn estimator API, \n",
    "    such that GridSearch can be used.\n",
    "    \n",
    "    Attributes:\n",
    "        See parameters of constructor! Additional attributes are:\n",
    "        \n",
    "        size_input: Number of neurons in input and output layer.\n",
    "        inputs: The input tensor (containing an input batch).\n",
    "        activity_trace_h[1,2,3]: The smoothed activation trace of hidden units.\n",
    "        h[1,2,3]: Tensor, containing the hidden activity.\n",
    "        output: Output tensor.\n",
    "        loss: The loss tensor (a scalar, that measures performance on current input).\n",
    "        sess: The tensorflow session.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=1.0, alpha_s=0.5, alpha_p=0.5, alpha_b=0.5, sparsity=0.5, \\\n",
    "                 gamma=0.99, size_h1=200, size_h2=30, batch_size=32, num_epochs=10000):\n",
    "        \"\"\"Construct the neural network using the given parameters.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Initial learning rate, which is halved in every evaluation \n",
    "                epoch.\n",
    "            alpha_s: Trade-off parameter for sparsity regularizer.\n",
    "            alpha_p: Trade-off parameter for participation regularizer.\n",
    "            alpha_b: Trade-off parameter for binary activation regularizer.\n",
    "            sparsity: Sparsity level that should be enforced within a population (layer).\n",
    "            gamma: Parameter to control exponential smoothing of activation traces.\n",
    "            size_h1: Number of neurons in first hidden layer.\n",
    "            size_h2: Number of neurons in second hidden layer (middle layer).\n",
    "            batch_size: Size of mini batches.\n",
    "            num_epochs: Number of training epochs.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha_s = alpha_s\n",
    "        self.alpha_p = alpha_p\n",
    "        self.alpha_b = alpha_b\n",
    "        self.sparsity = sparsity\n",
    "        self.gamma = gamma\n",
    "        self.size_input = 784\n",
    "        self.size_h1 = size_h1\n",
    "        self.size_h2 = size_h2\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def _build_network(self):\n",
    "        alpha_s = self.alpha_s\n",
    "        alpha_p = self.alpha_p\n",
    "        alpha_b = self.alpha_b\n",
    "        sparsity = self.sparsity\n",
    "        participation = (1-sparsity)\n",
    "        gamma = self.gamma\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        ## Build Network.\n",
    "        \n",
    "        # Network Inputs. Note that activity traces are maintained outside of the scope\n",
    "        # of backprop and therefore have to be network inputs.\n",
    "        self.inputs = tf.placeholder(tf.float32, shape=[None, self.size_input])\n",
    "        self._lambda = tf.placeholder(tf.float32, shape=[]) # Learning rate\n",
    "\n",
    "        self.activity_trace_h1 = tf.placeholder(tf.float32, shape=[self.size_h1])\n",
    "        self.activity_trace_h2 = tf.placeholder(tf.float32, shape=[self.size_h2])\n",
    "        self.activity_trace_h3 = tf.placeholder(tf.float32, shape=[self.size_h1])\n",
    "\n",
    "\n",
    "        # Create a five layer fully-connected network:\n",
    "        # size_input -> size_h1 -> size_h2 -> size_h1 -> size_input\n",
    "        # We use sigmoid units, as they are more similar to prob. binomial \n",
    "        # neurons as e.g. ReLUs.\n",
    "        self.h1 = tf.contrib.layers.fully_connected(self.inputs, \\\n",
    "            self.size_h1, activation_fn=tf.nn.sigmoid)\n",
    "        self.h2 = tf.contrib.layers.fully_connected(self.h1, \\\n",
    "            self.size_h2, activation_fn=tf.nn.sigmoid)\n",
    "        self.h3 = tf.contrib.layers.fully_connected(self.h2, \\\n",
    "            self.size_h1, activation_fn=tf.nn.sigmoid)\n",
    "        self.outputs = tf.contrib.layers.fully_connected(self.h3, \\\n",
    "            self.size_input, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "        ## Compute Loss.\n",
    "        # Reconstruction Error\n",
    "        # As the other loss terms (e.g. sparsity) are normalized to layer sizes, \n",
    "        # we should do this here too.\n",
    "        # Note, that the function L2 loss doesn't use the sqrt op.\n",
    "        self._rec_err = tf.nn.l2_loss(self.outputs-self.inputs) / float(self.size_input)\n",
    "\n",
    "        # Add sparsity within layer to loss\n",
    "        # (We only consider hidden layers, as the other layer shall be fully \n",
    "        # governed by the input)\n",
    "        self._loss_s = 0\n",
    "        # Compute binary loss.\n",
    "        self._loss_b = 0\n",
    "        for layer in [self.h1,self.h2,self.h3]:\n",
    "            n_l = float(int(layer.shape[1]))\n",
    "            self._loss_s += tf.reduce_mean(tf.reduce_sum(layer, 1)/n_l - participation)**2\n",
    "            \n",
    "            self._loss_b += tf.reduce_mean(tf.pow(layer,2)*tf.pow(layer-1,2))\n",
    "\n",
    "        # Add participation loss for all neurons\n",
    "        self._loss_p = 0\n",
    "        activity_traces = [self.activity_trace_h1, self.activity_trace_h2, \\\n",
    "                           self.activity_trace_h3]\n",
    "        x_s = sshift(participation) # sigmoid shift\n",
    "        for ii, layer in enumerate([self.h1,self.h2,self.h3]):\n",
    "            n_l = float(int(layer.shape[1]))\n",
    "            a_trace = activity_traces[ii]\n",
    "\n",
    "            ## Simple loss (unstable minima)\n",
    "            #self._loss_p += tf.reduce_mean(tf.reduce_sum( \\\n",
    "            #    (gamma*a_trace + (1-gamma)*layer - participation)**2, 1)/n_l)\n",
    "\n",
    "            ## Loss with minimas relatively stable around 0 or 1 \n",
    "            ## (depending on trace error)\n",
    "            # Root of the simple loss formulation.\n",
    "            root = - 1/(1-gamma) * (gamma*a_trace - participation)\n",
    "            sigval = sigmoid(participation - a_trace, x_s)\n",
    "            self._loss_p += tf.reduce_mean(tf.reduce_sum( \\\n",
    "                (gamma*a_trace + (1-gamma)*layer - participation + \\\n",
    "                 (1-gamma) * (root - sigval))**2, 1)/n_l)\n",
    "\n",
    "        self.loss = self._rec_err + alpha_s * self._loss_s + alpha_p * self._loss_p + \\\n",
    "            alpha_b * self._loss_b\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Train the model implemented by this class.\n",
    "        \n",
    "        NOTE, the input parameters are ignored. They just remain to ensure \n",
    "        compatibility with the sklearn framework. Instead, the MNIST training\n",
    "        set is used.\n",
    "        \n",
    "        Args:\n",
    "            X: array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "                The training input samples.\n",
    "            y: array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
    "                The target values (class labels in classification, real numbers in\n",
    "                regression).\n",
    "                \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        #X, y = check_X_y(X, y)\n",
    "        \n",
    "        self._build_network()\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        train_step = tf.train.GradientDescentOptimizer( \\\n",
    "            learning_rate=self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Initial activation traces.\n",
    "        at_h1 = np.ones(self.size_h1) * (1-self.sparsity)\n",
    "        at_h2 = np.ones(self.size_h2) * (1-self.sparsity)\n",
    "        at_h3 = np.ones(self.size_h1) * (1-self.sparsity)\n",
    "\n",
    "        for i in range(self.num_epochs):\n",
    "            batch = mnist.train.next_batch(self.batch_size)\n",
    "\n",
    "            # Decay learning rate.\n",
    "            if i % 1000 == 0:\n",
    "                self.learning_rate /= 2\n",
    "\n",
    "            # Run backprop\n",
    "            train_step.run( \\\n",
    "                feed_dict={self.inputs: batch[0], self._lambda: self.learning_rate, \\\n",
    "                           self.activity_trace_h1: at_h1, self.activity_trace_h2: at_h2, \\\n",
    "                           self.activity_trace_h3: at_h3})\n",
    "\n",
    "            # Update activity traces\n",
    "            activity_h1, activity_h2, activity_h3 = self.sess.run( \\\n",
    "                [self.h1, self.h2, self.h3], feed_dict={self.inputs: batch[0], \\\n",
    "                self._lambda: self.learning_rate, self.activity_trace_h1: at_h1, \\\n",
    "                self.activity_trace_h2: at_h2, self.activity_trace_h3: at_h3})\n",
    "            # We update the traces by the mean activity achieved in the current batch\n",
    "            activity_h1 = np.mean(activity_h1, axis=0)\n",
    "            activity_h2 = np.mean(activity_h2, axis=0)\n",
    "            activity_h3 = np.mean(activity_h3, axis=0)\n",
    "\n",
    "            gamma = self.gamma\n",
    "            at_h1 = gamma * at_h1 + (1-gamma) * activity_h1\n",
    "            at_h2 = gamma * at_h2 + (1-gamma) * activity_h2\n",
    "            at_h3 = gamma * at_h3 + (1-gamma) * activity_h3\n",
    "    \n",
    "        self._at_h1 = at_h1\n",
    "        self._at_h2 = at_h2\n",
    "        self._at_h3 = at_h3\n",
    "    \n",
    "        # Return the estimator\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" This function implements no logic, as it is simply reqoired by the estimator template.\n",
    "        \n",
    "        Args:\n",
    "            X: array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "                The training input samples.\n",
    "                \n",
    "        Returns:\n",
    "            y: array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
    "                The target values (class labels in classification, real numbers in\n",
    "                regression).\n",
    "        \"\"\"\n",
    "        raise\n",
    "        X = check_array(X)\n",
    "        return X[:, 0]**2\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        \"\"\" Compute the loss on the full test set.\n",
    "        \n",
    "        NOTE, the input parameters are ignored. They just remain to ensure \n",
    "        compatibility with the sklearn framework. Instead, the MNIST test\n",
    "        set is used.\n",
    "        \n",
    "        Args:\n",
    "            X: array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "                The training input samples.\n",
    "            y: array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
    "                The target values (class labels in classification, real numbers in\n",
    "                regression).\n",
    "                \n",
    "        Returns:\n",
    "            The loss computed over the complete test set.\n",
    "        \"\"\"\n",
    "        test_images = mnist.test.images\n",
    "        loss = self.sess.run(self.loss, feed_dict={\n",
    "            self.inputs: test_images, self._lambda: self.learning_rate, \\\n",
    "           self.activity_trace_h1: self._at_h1, self.activity_trace_h2: self._at_h2, \\\n",
    "           self.activity_trace_h3: self._at_h3})\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the parameter grid\n",
    "\n",
    "Here we initiate the actual gridsearch and define the parameters through which we sweep. We then output a sorted list of tuples (params, loss). Loss is the loss value achieved in one training (we start one training per point in our parameter grid), and by which the list is sorted. Params are the hyerparameters used for this particular model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'learning_rate':[0.1,1.0,10,100],\n",
    "    'alpha_s':[0.5,1.0,2.0,5.0,10.0,100.0],\n",
    "    'alpha_p':[0.5,1.0,2.0,5.0,10.0,100.0],\n",
    "    'alpha_b':[0.5,1.0,2.0,5.0,10.0,100.0],\n",
    "    'sparsity':[0.25,0.33,0.5,0.66,0.75],\n",
    "    'gamma':[0.99],\n",
    "    'size_h1':[200],\n",
    "    'size_h2':[30,45,60],\n",
    "    'batch_size':[32],\n",
    "    'num_epochs':[10000]\n",
    "}\n",
    "\n",
    "ae = AutoencoderMNIST()\n",
    "# Note, that tensorflow gpu version might cause trouble when using multithreading.\n",
    "# Note, we are not using cross-validation.\n",
    "clf = GridSearchCV(ae, parameters, cv=ShuffleSplit(test_size=0.20, n_splits=1, random_state=0), \\\n",
    "                  n_jobs=-1, verbose=1)\n",
    "clf.fit([0,0], [0,0])\n",
    "\n",
    "results = list(zip(clf.cv_results_['params'], clf.cv_results_['mean_test_score']))\n",
    "results = sorted(results, key=lambda x: x[1])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( clf.cv_results_, open( \"gridsearch_results_11-19-17.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 33\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 0.5, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 30, 'sparsity': 0.75}, 90.057632446289062)\n",
      "Rank: 35\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 1.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 30, 'sparsity': 0.75}, 90.190444946289062)\n",
      "Rank: 51\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 0.5, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 30, 'sparsity': 0.66}, 92.385345458984375)\n",
      "Rank: 60\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 2.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 30, 'sparsity': 0.75}, 94.083953857421875)\n",
      "Rank: 68\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 1.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 30, 'sparsity': 0.66}, 94.913589477539062)\n",
      "Rank: 75\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 2.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 30, 'sparsity': 0.66}, 96.473663330078125)\n",
      "Rank: 90\n",
      "({'alpha_b': 0.5, 'alpha_p': 1.0, 'alpha_s': 0.5, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 30, 'sparsity': 0.75}, 98.136337280273438)\n",
      "Rank: 104\n",
      "({'alpha_b': 0.5, 'alpha_p': 2.0, 'alpha_s': 0.5, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 30, 'sparsity': 0.75}, 101.0103759765625)\n",
      "Rank: 109\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 5.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 30, 'sparsity': 0.75}, 101.39855194091797)\n",
      "Rank: 110\n",
      "({'alpha_b': 0.5, 'alpha_p': 5.0, 'alpha_s': 0.5, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 30, 'sparsity': 0.75}, 101.52214050292969)\n"
     ]
    }
   ],
   "source": [
    "with open(\"gridsearch_results_11-19-17.p\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "results = list(zip(data['params'], data['mean_test_score']))\n",
    "results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "num = 0\n",
    "for i, r in enumerate(results):\n",
    "    if r[0]['size_h2'] == 30:\n",
    "        print(\"Rank: %d\" % i)\n",
    "        print(r)\n",
    "        num += 1\n",
    "        if num == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 6\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 0.5, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 45, 'sparsity': 0.75}, 80.040061950683594)\n",
      "Rank: 9\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 0.5, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 45, 'sparsity': 0.66}, 80.479019165039062)\n",
      "Rank: 21\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 1.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 45, 'sparsity': 0.75}, 85.653648376464844)\n",
      "Rank: 25\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 5.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 45, 'sparsity': 0.75}, 86.641769409179688)\n",
      "Rank: 26\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 1.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 45, 'sparsity': 0.66}, 87.452468872070312)\n",
      "Rank: 28\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 10.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 45, 'sparsity': 0.75}, 88.144744873046875)\n",
      "Rank: 31\n",
      "({'alpha_b': 0.5, 'alpha_p': 100.0, 'alpha_s': 10.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 45, 'sparsity': 0.66}, 89.032958984375)\n",
      "Rank: 36\n",
      "({'alpha_b': 0.5, 'alpha_p': 5.0, 'alpha_s': 1.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 45, 'sparsity': 0.75}, 90.275497436523438)\n",
      "Rank: 37\n",
      "({'alpha_b': 0.5, 'alpha_p': 10.0, 'alpha_s': 2.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 45, 'sparsity': 0.75}, 90.373847961425781)\n",
      "Rank: 40\n",
      "({'alpha_b': 0.5, 'alpha_p': 2.0, 'alpha_s': 1.0, 'batch_size': 32, 'gamma': 0.99, 'learning_rate': 10, 'num_epochs': 10000, 'size_h1': 200, 'size_h2': 45, 'sparsity': 0.75}, 91.113235473632812)\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for i, r in enumerate(results):\n",
    "    if r[0]['size_h2'] == 45:\n",
    "        print(\"Rank: %d\" % i)\n",
    "        print(r)\n",
    "        num += 1\n",
    "        if num == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 0.5,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  75.356842041015625),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 5.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.66},\n",
       "  75.595344543457031),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 2.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  77.370628356933594),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 0.5,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.66},\n",
       "  77.700645446777344),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 5.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  77.709999084472656),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 10.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  77.712295532226562),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 0.5,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 45,\n",
       "   'sparsity': 0.75},\n",
       "  80.040061950683594),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 10.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.66},\n",
       "  80.047286987304688),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 1.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.66},\n",
       "  80.167762756347656),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 0.5,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 45,\n",
       "   'sparsity': 0.66},\n",
       "  80.479019165039062),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 10.0,\n",
       "   'alpha_s': 5.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  81.484161376953125),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 2.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.66},\n",
       "  82.636436462402344),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 0.5,\n",
       "   'alpha_s': 1.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  82.998634338378906),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 1.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  83.271568298339844),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 5.0,\n",
       "   'alpha_s': 5.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  84.2662353515625),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 2.0,\n",
       "   'alpha_s': 1.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  84.479393005371094),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 10.0,\n",
       "   'alpha_s': 1.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  84.623588562011719),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 0.5,\n",
       "   'alpha_s': 5.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  84.737197875976562),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 5.0,\n",
       "   'alpha_s': 1.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  84.759140014648438),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 1.0,\n",
       "   'alpha_s': 5.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  84.792900085449219),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 5.0,\n",
       "   'alpha_s': 2.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  85.083511352539062),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 1.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 45,\n",
       "   'sparsity': 0.75},\n",
       "  85.653648376464844),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 1.0,\n",
       "   'alpha_s': 2.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  85.800636291503906),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 1.0,\n",
       "   'alpha_s': 10.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  86.3060302734375),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 10.0,\n",
       "   'alpha_s': 0.5,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  86.587234497070312),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 5.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 45,\n",
       "   'sparsity': 0.75},\n",
       "  86.641769409179688),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 1.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 45,\n",
       "   'sparsity': 0.66},\n",
       "  87.452468872070312),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 10.0,\n",
       "   'alpha_s': 2.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  87.955734252929688),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 10.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 45,\n",
       "   'sparsity': 0.75},\n",
       "  88.144744873046875),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 2.0,\n",
       "   'alpha_s': 10.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  88.171241760253906),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 10.0,\n",
       "   'alpha_s': 10.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  88.820106506347656),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 10.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 45,\n",
       "   'sparsity': 0.66},\n",
       "  89.032958984375),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 5.0,\n",
       "   'alpha_s': 0.5,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  89.673576354980469),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 0.5,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 30,\n",
       "   'sparsity': 0.75},\n",
       "  90.057632446289062),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 5.0,\n",
       "   'alpha_s': 5.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.66},\n",
       "  90.102523803710938),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 100.0,\n",
       "   'alpha_s': 1.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 30,\n",
       "   'sparsity': 0.75},\n",
       "  90.190444946289062),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 5.0,\n",
       "   'alpha_s': 1.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 45,\n",
       "   'sparsity': 0.75},\n",
       "  90.275497436523438),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 10.0,\n",
       "   'alpha_s': 2.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 45,\n",
       "   'sparsity': 0.75},\n",
       "  90.373847961425781),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 2.0,\n",
       "   'alpha_s': 0.5,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.75},\n",
       "  90.48583984375),\n",
       " ({'alpha_b': 0.5,\n",
       "   'alpha_p': 2.0,\n",
       "   'alpha_s': 5.0,\n",
       "   'batch_size': 32,\n",
       "   'gamma': 0.99,\n",
       "   'learning_rate': 10,\n",
       "   'num_epochs': 10000,\n",
       "   'size_h1': 200,\n",
       "   'size_h2': 60,\n",
       "   'sparsity': 0.66},\n",
       "  90.6708984375)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:40]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
